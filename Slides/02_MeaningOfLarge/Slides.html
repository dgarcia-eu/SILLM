<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>The Social Informatics of Large Language Models</title>
    <meta charset="utf-8" />
    <meta name="author" content="David Garcia &amp; Indira Sen    University of Konstanz" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
    <link rel="stylesheet" href="libs/footer.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# The Social Informatics of Large Language Models
]
.author[
### David Garcia &amp; Indira Sen <br><br> <em>University of Konstanz</em>
]
.date[
### 02 - What does large mean?
]

---






layout: true

&lt;div class="my-footer"&gt;&lt;span&gt;David Garcia - The Social Informatics of Large Language Models&lt;/span&gt;&lt;/div&gt; 

---

# Outline

## 1. Model size and the bias/variance tradeoff

## 2. The double descent

## 3. A small models: chaotic logistic maps

## 4. When one parameter is enough

---

# Machine Learning
.pull-left[.center[
&lt;img src="figs/Machin1.jpeg" width="450" /&gt;
]]
.pull-right[
# Antonio Machin
]
---

# Machine Learning
.pull-left[.center[
&lt;img src="figs/Machin2.jpeg" width="450" /&gt;
]]
.pull-right[
# Antonio Machin learning
]
---

# Machine Learning
.pull-left[.center[
&lt;img src="figs/Machin3.jpeg" width="450" /&gt;
]]
.pull-right[
# Antonio Machin learning Machine Learning
]
---

### Machine learning Antonio Machin learning Machine Learning
.center[
&lt;img src="figs/Machin4.jpeg" width="620" /&gt;
] https://twitter.com/TeoSeHaceHacker
---

# Machine learning problems

Training examples:

`$$(x_1, y_1), ... , (x_n, y_n) \in \mathbb R^d \times \mathbb R$$`
In the previous example:
- `\(x_i\)` is a vector of features of the image
- `\(y_i\)` is a class: it contains *Machin learning Machine Learning* (1) or not (0)

`\(y_i\)` can also be a number (regression) or a class from a larger set

Training data is assumed to be representative (e.g. taken at random) from a larger distrbution of interest (where we want to apply the classifier)

---

# Training and test risk

A machine learning model is a predictor for unseen data
$$ h_n: \mathbb R^d \rightarrow \mathbb R $$


.pull-left[
Fitting: minimize training risk (error) based on loss function `\(l\)`
$$ \frac{1}{n} \sum_{i=1}^n l(h(x_i),y_i) $$
]

.pull-right[
Common loss functions: 
- regression: `\(l(y',y) = (y'-y)^2\)`
- classification: `\(l(y',y)  = \mathbb 1 _{y' \neq y}\)`]

Machine Learning aims to minimize **test risk**: same mean loss over an unseen test dataset from the same population as the training dataset
---

# Capacity and model size

The predictor function `\(h_n\)` belongs to a larger function class `\(\mathcal H\)` where all possible fits we can do exist.

The **capacity** of `\(\mathcal H\)` measures how large `\(\mathcal H\)` is

  - The capacity is often approximated as the **number of parameters** of `\(h_n\)`

  - *small* models come from low-capacity `\(\mathcal H\)`, i.e. have few parameters

  - *large* models come from high-capacity `\(\mathcal H\)`, i.e. have many parameters

[Reconciling modern machine-learning practice and the classical bias–variance trade-off. Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. PNAS (2019)](https://www.pnas.org/doi/10.1073/pnas.1903070116)

---

# Overfitting and Underfitting
.center[![:scale 80%](figs/Underfit.svg)]
https://www.mathworks.com/discovery/overfitting.html

---

# Bias-Variance tradeoff
.center[![:scale 65%](figs/Overfit.png)]
Bias: underfitting (min. training risk), Variance: overfitting (min. test risk)
---

# Overparametrization

## 1. Model size and the bias/variance tradeoff

## *2. The double descent*

## 3. A small models: chaotic logistic maps

## 4. When one parameter is enough

---
# The double descent
![](figs/Overparametrize.png)
For some problems and models (e.g. deep neural networks), test risk might decrease for extremely models. Training risk becomes zero after capacity=n.

---

# Occam's razor?

.center[*"The simplest explanation is usually the best one."*]

What if a simple model is not necessarily the one with the fewer parameters?

`\(l_2\)` norm, also known as Eucledian norm:
`$$l_2 = \sum_j^P x_j^2$$`
- A model with more parameters could generalize better if their `\(l_2\)` norm is smaller than models with less parameters

- Does the `\(l_2\)` norm also decrease in the second descent?

---

# MNIST as a study in double descent

.center[![](figs/MNIST.png)]

MNIST is a database of handwritten digits. A machine learning task is to classify the image of the hand-written characters to their corresponding letters/digits

---

# MNIST double descent with NN and RF

.pull-left[
![:scale 95%](figs/RNN.png)
]

.pull-right[
![](figs/RF.png)
]

---
# MNIST double descent with RFF model

.center[![:scale 90%](figs/RFF.png)]

---
# Norm in RFF for MNIST
.center[![:scale 88%](figs/RFF2.png)]

---

.center[![:scale 90%](figs/overfittingMeme.png)]

---

# Some caveats

- **Beware of overselling of large models**

- The double descent exists in some problems but it is far from true that it exists in all problems

- Overparametrization requires evidence
  - Careful construction and handling of test samples
  - Out-Of-Domain (OOD) tests
  - If possible: analysis of performance as a function of model size

---

# The Von Neumann Challenge

*"With four parameters I can fit an elephant, and with five I can make him wiggle his trunk"* .right[Attributed to John Von Neumann by Enrico Fermi]
.center[![:scale 70%](figs/Elephant.png)]
[Drawing an elephant with four complex parameters . Jürgen Mayer; Khaled Khairy; Jonathon Howard. American Journal of Physics (2010)](https://pubs.aip.org/aapt/ajp/article-abstract/78/6/648/1042069/Drawing-an-elephant-with-four-complex-parameters?redirectedFrom=fulltext)

---

# Logistic Maps and Chaos

## 1. Model size and the bias/variance tradeoff

## 2. The double descent

## *3. A small models: chaotic logistic maps*

## 4. When one parameter is enough

---

# Logistic maps

Iterative function of one-dimensional values that are calculated based on only the previous value. For example:

`$$x_{n+1}=\lambda x_{n}(1-x_{n})\quad\text{with}\quad n=0,1,2,3…$$`
Behavior depends on `\(\lambda\)` and the initial condition `\(x_0\)`,

For example, for `\(\lambda=0.920\)` and `\(x_0=0.023\)`

`\(x_{0...}=\)` 
0.023, 0.021, 0.019, 0.017, 0.015, 0.014, 0.012, 0.011, 0.010, 0.009, 0.009, 0.008, 0.007, 0.006, 0.006, 0.005, 0.005,...

https://www.complexity-explorables.org/flongs/logistic/

---

# Iterating over a map

.center[![:scale 60%](figs/Map1.svg)]

---

# Iterating over a map

.center[![:scale 60%](figs/Map2.svg)]

---

# Iterating over a map: periodic example

.center[![:scale 60%](figs/Map3.svg)]

---

# Iterating over a map: aperiodic example

.center[![:scale 60%](figs/Map5.svg)]

---
# Chaos: sensitivity to initial conditions

.center[![:scale 49%](figs/Map4.svg) ![:scale 50%](figs/Map6.svg)]

---
# Chaotic time series
.center[![:scale 85%](figs/TS.png)]
.center[https://www.complexity-explorables.org/flongs/logistic/]

---


# When one parameter is enough

## 1. Model size and the bias/variance tradeoff

## 2. The double descent

## 3. A small models: chaotic logistic maps

## *4. When one parameter is enough*

---

# Logistic maps as models of scatter plots
.center[![:scale 100%](figs/Piantadosi.png)]

[One Parameter is Always Enough. Steven Piantadosi. AIP advances (2018)](https://pubs.aip.org/aip/adv/article/8/9/095118/22887/One-parameter-is-always-enough)

---

#Fitting any scatter plot

.center[![:scale 95%](figs/Fits.png)]
For every scatter plot to fit, there is an initial value ( `\(\theta\)` ) of the chaotic iterative map that generates it

---

# Reflecting on one paremeter is enough

- If we can fit anything with just one parameter, what is the catch?

--

- If we fit a training dataset of a classification with this method, how good do you think will be the model in a test dataset?

--

- In LLMs used as chatbots, where could we have an infinite-precision parameter `\(\theta\)`?

--

- How do we avoid overfitting on `\(\theta\)` when using a LLM?

--

- What have we learned today about model sizes?



    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script src="libs/perc.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
