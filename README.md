# The Social Informatics of Large Language Models
[David Garcia](http://dgarcia.eu), 2025

This course will cover the latest research on generative pre-trained large-language 
models (LLMs) from a social informatics approach. It will be centered around readings 
and practical exercises to understand the design and training of the models and to be 
able to actively use them in a Python programming environment. The course will cover 
current methods to quantify behavioral patterns in generative LLM, apply them for social 
scientific tasks, such as zero-shot or few-shot classification of text, and inform furthe
research. The course will cover the current and potential future role of 
generative LLMs in society and how biases and representation issues affect the training
and design of generative LLMs.


# Learning objectives

Upon completion of this course, students will be able to:

- Critically reflect on the validity, stability, and accuracy of large language 
models and their role in society

- Systematically use large language models as software tools integrated with 
their own code

- Design social and behavioral science tasks for large language models to perform 
and evaluate their performance and reliability

- Audit the behavior of large language models to detect inconsistencies and other 
issues when compared to human behavior
    
    
# Course structure

The course is composed of lectures and interactive sessions including debates 
and flipped classroom sessions. Each session is based on recent research and technological
topics with readings to prepare in advance. Along the course, there is a series of 
tutorials to develop skills on LLMs. The course has two assignments as exercises to 
replicate previous research on LLMs.


# Course topics
**Week 1: [Introduction and Motivation](https://dgarcia-eu.github.io/SILLM/Slides/01_Intro/Slides.html)**
  
  - Tutorial 0: course set up, model access, proprietary APIs, local models
  
**Week 2: [What does large mean?](https://dgarcia-eu.github.io/SILLM/Slides/02_MeaningOfLarge/Slides.html)**

  - Reading 1: [Reconciling modern machine-learning practice and the classical bias–variance trade-off](https://www.pnas.org/doi/10.1073/pnas.1903070116)
  - Reading 2: [One Parameter is Always Enough](http://colala.berkeley.edu/papers/piantadosi2018one.pdf)
  - [Tutorial: Introduction to content analysis with zero and few-shot labeling](https://github.com/dgarcia-eu/SILLM/tree/main/Tutorials/tutorial_1)

**Week 3: Natural Language Processing basics**
<!--(https://github.com/dgarcia-eu/SILLM/blob/main/Slides/03_nlp_basics_1.pdf)-->
  
  - [Tutorial: NLP basics](https://github.com/dgarcia-eu/SILLM/tree/main/Tutorials/tutorial_2)

**Week 4: Advanced concepts in Natural Language Processing**
<!--(https://github.com/dgarcia-eu/SILLM/blob/main/Slides/04_nlp_intermediate.pdf.pdf)-->
  
  - [Tutorial: Word embeddings](https://github.com/dgarcia-eu/SILLM/tree/main/Tutorials/tutorial_3)

**Week 5: Artificial General Intelligence and applications of LLMs**
<!--(https://github.com/dgarcia-eu/SILLM/blob/main/Slides/05_NLP_and_AGI.pdf)-->

  - Reading: [Sparks of Artificial General Intelligence: Early experiments with GPT-4](https://arxiv.org/abs/2303.12712)
  - [Tutorial: Fine tuning](https://github.com/dgarcia-eu/SILLM/tree/main/Tutorials/tutorial_4)

**Week 6: Content analysis with generative LLM**
<!--(https://github.com/dgarcia-eu/SILLM/blob/main/Slides/06_content_labeling.pdf.pdf)-->
  
  - Reading 1: [ChatGPT outperforms crowd workers for text-annotation tasks](https://www.pnas.org/doi/10.1073/pnas.2305016120)
  - Reading 2: [Chatbots Are Not Reliable Text Annotators](https://arxiv.org/abs/2311.05769)
  - [Tutorial: Content analysis part 2](https://github.com/dgarcia-eu/SILLM/tree/main/Tutorials/tutorial_5)

**Week 7: Social simulacra and generative agents**
  
  - Reading 1: [Social Simulacra: Creating Populated Prototypes for Social Computing Systems](https://arxiv.org/pdf/2208.04024.pdf)
  - Reading 2: [Simulating social media using large language models to evaluate alternative news feed algorithms](https://arxiv.org/pdf/2310.05984.pdf)
  - [Tutorial: Quantization and sharding](https://github.com/dgarcia-eu/SILLM/tree/main/Tutorials/tutorial_6)

**Week 8: [Prompt engineering](https://dgarcia-eu.github.io/SILLM/Slides/08_PromptEngineering/Slides.html)**
  
  - Reading 1: [Large Language Models Understand and Can Be Enhanced by Emotional Stimuli](https://arxiv.org/pdf/2307.11760.pdf)
  - Reading 2: [Evaluating and Mitigating Discrimination in Language Model Decisions](https://arxiv.org/pdf/2312.03689.pdf)
  - [Tutorial: Sentence embeddings](https://github.com/dgarcia-eu/SILLM/tree/main/Tutorials/tutorial_7)

**Week 9: Psychometrics of LLMs and LLM Behavior**
<!--(https://github.com/dgarcia-eu/SILLM/blob/main/Slides/09_bias_psychometrics_experiments.pdf.pdf):-->
  
  - Reading 1: [AI Psychometrics: Assessing the Psychological Profiles of Large Language Models Through Psychometric Inventories](https://journals.sagepub.com/doi/full/10.1177/17456916231214460)
  - Reading 2: [Using Large Language Models to Simulate Multiple Humans and Replicate Human Subject Studies](https://proceedings.mlr.press/v202/aher23a.html)
  - [Tutorial: Sentence embeddings 2](https://github.com/dgarcia-eu/SILLM/tree/main/Tutorials/tutorial_8)
  
**Week 10: Impact of LLMs in society**
<!--(https://github.com/dgarcia-eu/SILLM/blob/main/Slides/10_LLM_impact.pdf):-->
  
  - Reading 1: [US eating disorder helpline takes down AI chatbot over harmful advice](https://www.theguardian.com/technology/2023/may/31/eating-disorder-hotline-union-ai-chatbot-harm)
  - Reading 2: [Are large language models a threat to digital public goods? evidence from activity on stack overflow](https://arxiv.org/abs/2307.07367)
  - [Tutorial: Prompt engineering](http://github.com/dgarcia-eu/SILLM/tree/main/Tutorials/tutorial_9)

**Week 11: Critique and ethics of LLMs**
<!--(https://github.com/dgarcia-eu/SILLM/blob/main/Slides/11_Critique_of_LLMs.pdf)-->
  
  - Reading 1: [On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?](https://dl.acm.org/doi/10.1145/3442188.3445922)
  - Reading 2: [ChatGPT Is a Blurry JPEG of the Web](https://www.newyorker.com/tech/annals-of-technology/chatgpt-is-a-blurry-jpeg-of-the-web)
  - [Tutorial: LLM psychometrics](https://github.com/dgarcia-eu/SILLM/blob/main/Tutorials/tutorial_10_sillm_psychometrics.ipynb)

**Week 12: Course summary and preliminary project presentations**
  - [Tutorial: LLM security, jailbreaking and red-teaming](https://github.com/dgarcia-eu/SILLM/blob/main/Tutorials/tutorial_12_sillm_jailbreaking.pdf)

**Week 13-14: Project presentations**

# Assignments

- **[Assignment 1.1:  Replicating ‘People Make Better Edits: Measuring the Efficacy of LLM-Generated Counterfactually Augmented Data’](https://github.com/dgarcia-eu/SILLM/blob/main/Assignments/SILLM_assignment_1.1.md)**

- **[Assignment 1.2: LLMs for Training Data Generation - Counterfactual Augmentation](https://github.com/dgarcia-eu/SILLM/blob/main/Assignments/SILLM_assignment_1.2.md)**

- **[Assignment 2: Investigating LLM's Shared Values and their Implications](https://github.com/dgarcia-eu/SILLM/blob/main/Assignments/SILLM_assignment_2.pdf)**


Shout-out to [Indira Sen](https://indiiigo.github.io/) and [Mats Faulborn](https://github.com/MaFa211), who co-developed and co-taught this course in 2023.








