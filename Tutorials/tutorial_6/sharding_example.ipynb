{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM,AutoTokenizer\n",
    "from accelerate import load_checkpoint_and_dispatch\n",
    "from accelerate import Accelerator\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n",
    "\n",
    "\n",
    "save_directory='/content/model'\n",
    "accelerator = Accelerator()\n",
    "\n",
    "\n",
    "accelerator.save_model(model=model, save_directory=save_directory,max_shard_size='4GB')\n",
    "\n",
    "device_map={'':'gpu'}\n",
    "\n",
    "model = load_checkpoint_and_dispatch(\n",
    "model, checkpoint='/content/model/', device_map='auto', no_split_module_classes=['Block']\n",
    ")\n",
    "\n",
    "prompt = \"Hello Mistral!\"\n",
    "\n",
    "inputs = tokenizer(prompt, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=100, return_dict_in_generate=True, output_scores=True)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
